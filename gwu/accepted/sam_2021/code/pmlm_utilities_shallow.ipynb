{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nt5cAnU6UcRp",
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Overview\" data-toc-modified-id=\"Overview-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Overview</a></span></li><li><span><a href=\"#Helper-Functions\" data-toc-modified-id=\"Helper-Functions-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Helper Functions</a></span><ul class=\"toc-item\"><li><span><a href=\"#Data-preprocessing\" data-toc-modified-id=\"Data-preprocessing-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Data preprocessing</a></span><ul class=\"toc-item\"><li><span><a href=\"#Identifying-uncommon-variables\" data-toc-modified-id=\"Identifying-uncommon-variables-2.1.1\"><span class=\"toc-item-num\">2.1.1&nbsp;&nbsp;</span>Identifying uncommon variables</a></span></li><li><span><a href=\"#Identifying-identifiers\" data-toc-modified-id=\"Identifying-identifiers-2.1.2\"><span class=\"toc-item-num\">2.1.2&nbsp;&nbsp;</span>Identifying identifiers</a></span></li><li><span><a href=\"#Transforming-date-time-variables\" data-toc-modified-id=\"Transforming-date-time-variables-2.1.3\"><span class=\"toc-item-num\">2.1.3&nbsp;&nbsp;</span>Transforming date time variables</a></span></li><li><span><a href=\"#Identifying-missing-values\" data-toc-modified-id=\"Identifying-missing-values-2.1.4\"><span class=\"toc-item-num\">2.1.4&nbsp;&nbsp;</span>Identifying missing values</a></span></li><li><span><a href=\"#Identifying-categorical-variables-(features-and-target)\" data-toc-modified-id=\"Identifying-categorical-variables-(features-and-target)-2.1.5\"><span class=\"toc-item-num\">2.1.5&nbsp;&nbsp;</span>Identifying categorical variables (features and target)</a></span></li></ul></li><li><span><a href=\"#Data-visualization\" data-toc-modified-id=\"Data-visualization-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Data visualization</a></span><ul class=\"toc-item\"><li><span><a href=\"#Plot-the-scatter-plot-using-TSNE\" data-toc-modified-id=\"Plot-the-scatter-plot-using-TSNE-2.2.1\"><span class=\"toc-item-num\">2.2.1&nbsp;&nbsp;</span>Plot the scatter plot using TSNE</a></span></li><li><span><a href=\"#Separate-the-duplicated-class-from-the-original-class\" data-toc-modified-id=\"Separate-the-duplicated-class-from-the-original-class-2.2.2\"><span class=\"toc-item-num\">2.2.2&nbsp;&nbsp;</span>Separate the duplicated class from the original class</a></span></li><li><span><a href=\"#Separate-the-generated-class-from-the-original-class\" data-toc-modified-id=\"Separate-the-generated-class-from-the-original-class-2.2.3\"><span class=\"toc-item-num\">2.2.3&nbsp;&nbsp;</span>Separate the generated class from the original class</a></span></li></ul></li><li><span><a href=\"#Training,-validation-and-test\" data-toc-modified-id=\"Training,-validation-and-test-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Training, validation and test</a></span><ul class=\"toc-item\"><li><span><a href=\"#Getting-the-predefined-split-cross-validator\" data-toc-modified-id=\"Getting-the-predefined-split-cross-validator-2.3.1\"><span class=\"toc-item-num\">2.3.1&nbsp;&nbsp;</span>Getting the predefined split cross-validator</a></span></li><li><span><a href=\"#Training,-validation-and-test\" data-toc-modified-id=\"Training,-validation-and-test-2.3.2\"><span class=\"toc-item-num\">2.3.2&nbsp;&nbsp;</span>Training, validation and test</a></span></li></ul></li></ul></li><li><span><a href=\"#Reference\" data-toc-modified-id=\"Reference-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Reference</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pKlbheSUaIwR"
   },
   "source": [
    "<b>\n",
    "\n",
    "<p>\n",
    "<center>\n",
    "<font size=\"5\">\n",
    "Helper Functions for an End-to-End Pipeline for Diagnosing Cancer in BCW\n",
    "</font>\n",
    "</center>\n",
    "</p>\n",
    "</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rT2SKHw2zlEi"
   },
   "source": [
    "# Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "asZWLrJKzlEj"
   },
   "source": [
    "This notebook includes the code for the helper functions for an end-to-end pipeline for diagnosing cancer in BCW, introduced in the following paper submitted to Journal *Statistics in Medicine*:\n",
    "- \"Improving Classification Accuracy by Data Augmentation using Generative Adversarial Networks\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WJ-IbZqAgILJ"
   },
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "attLPyl5UcSQ"
   },
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gXFG7kb6VEJt"
   },
   "source": [
    "### Identifying uncommon variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OYu9QUjLVEJu"
   },
   "source": [
    "The code below shows how to find common variables between the training, validation and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "A71CAzRvVEJu"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def common_var_checker(df_train, df_val, df_test, target):\n",
    "    \"\"\"\n",
    "    The common variables checker\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df_train : the dataframe of training data\n",
    "    df_val : the dataframe of validation data\n",
    "    df_test : the dataframe of test data\n",
    "    target : the name of the target\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    The dataframe of common variables between the training, validation and test data\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get the dataframe of common variables between the training, validation and test data\n",
    "    df_common_var = pd.DataFrame(np.intersect1d(np.intersect1d(df_train.columns, df_val.columns), np.union1d(df_test.columns, [target])),\n",
    "                                 columns=['common var'])\n",
    "                \n",
    "    return df_common_var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k6bYp21EUcTg"
   },
   "source": [
    "### Identifying identifiers\n",
    "The code below shows how to find *Identifiers* (a feature whose value is unique for each sample) from data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Ujt9tr72UcTh"
   },
   "outputs": [],
   "source": [
    "def id_checker(df, dtype='float'):\n",
    "    \"\"\"\n",
    "    The identifier checker\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : dataframe\n",
    "    dtype : the data type identifiers cannot have, 'float' by default\n",
    "            i.e., if a feature has this data type, it cannot be an identifier\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    The dataframe of identifiers\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get the dataframe of identifiers\n",
    "    df_id = df[[var for var in df.columns\n",
    "                # If the data type is not dtype\n",
    "                if (df[var].dtype != dtype\n",
    "                    # If the value is unique for each sample\n",
    "                    and df[var].nunique(dropna=True) == df[var].notnull().sum())]]\n",
    "    \n",
    "    return df_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YkvArMXuVEJ4"
   },
   "source": [
    "### Transforming date time variables\n",
    "The code below shows how to transform date time variables into the following 6 datetime types:\n",
    "- year\n",
    "- month\n",
    "- day\n",
    "- hour\n",
    "- minute\n",
    "- second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "6_uL07UzVEJ4"
   },
   "outputs": [],
   "source": [
    "def datetime_transformer(df, datetime_vars):\n",
    "    \"\"\"\n",
    "    The datetime transformer\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : the dataframe\n",
    "    datetime_vars : the datetime variables\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    The dataframe where datetime_vars are transformed into the following 6 datetime types:\n",
    "    year, month, day, hour, minute and second\n",
    "    \"\"\"\n",
    "    \n",
    "    # The dictionary with key as datetime type and value as datetime type operator\n",
    "    dict_ = {'year'   : lambda x : x.dt.year,\n",
    "             'month'  : lambda x : x.dt.month,\n",
    "             'day'    : lambda x : x.dt.day,\n",
    "             'hour'   : lambda x : x.dt.hour,\n",
    "             'minute' : lambda x : x.dt.minute,\n",
    "             'second' : lambda x : x.dt.second}\n",
    "    \n",
    "    # Make a copy of df\n",
    "    df_datetime = df.copy(deep=True)\n",
    "    \n",
    "    # For each variable in datetime_vars\n",
    "    for var in datetime_vars:\n",
    "        # Cast the variable to datetime\n",
    "        df_datetime[var] = pd.to_datetime(df_datetime[var])\n",
    "        \n",
    "        # For each item (datetime_type and datetime_type_operator) in dict_\n",
    "        for datetime_type, datetime_type_operator in dict_.items():\n",
    "            # Add a new variable to df_datetime where:\n",
    "            # the variable's name is var + '_' + datetime_type\n",
    "            # the variable's values are the ones obtained by datetime_type_operator\n",
    "            df_datetime[var + '_' + datetime_type] = datetime_type_operator(df_datetime[var])\n",
    "            \n",
    "    # Remove datetime_vars from df_datetime\n",
    "    df_datetime = df_datetime.drop(columns=datetime_vars)\n",
    "                \n",
    "    return df_datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZaTMyKlNUcTx"
   },
   "source": [
    "### Identifying missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6E_PkWBOUcUG"
   },
   "source": [
    "The code below shows how to find variables with NaN, their proportion of NaN and data type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "LxgqyxZEUcUH"
   },
   "outputs": [],
   "source": [
    "def nan_checker(df):\n",
    "    \"\"\"\n",
    "    The NaN checker\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : the dataframe\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    The dataframe of variables with NaN, their proportion of NaN and data type\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get the dataframe of variables with NaN, their proportion of NaN and data type\n",
    "    df_nan = pd.DataFrame([[var, df[var].isna().sum() / df.shape[0], df[var].dtype]\n",
    "                           for var in df.columns if df[var].isna().sum() > 0],\n",
    "                          columns=['var', 'proportion', 'dtype'])\n",
    "    \n",
    "    # Sort df_nan in accending order of the proportion of NaN\n",
    "    df_nan = df_nan.sort_values(by='proportion', ascending=False).reset_index(drop=True)\n",
    "    \n",
    "    return df_nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ACRWcDG1UcUJ"
   },
   "source": [
    "### Identifying categorical variables (features and target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K95U7PEQUcUJ"
   },
   "source": [
    "The code below shows how to find categorical variables (whose data type is dtype) and their number of unique value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "7VFbpqrbUcUK"
   },
   "outputs": [],
   "source": [
    "def cat_var_checker(df, dtype='object'):\n",
    "    \"\"\"\n",
    "    The categorical variable checker\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : the dataframe\n",
    "    dtype : the data type categorical variables should have, 'object' by default\n",
    "            i.e., if a variable has this data type, it should be a categorical variable\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    The dataframe of categorical variables and their number of unique value\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get the dataframe of categorical variables and their number of unique value\n",
    "    df_cat = pd.DataFrame([[var, df[var].nunique(dropna=False)]\n",
    "                           # If the data type is dtype\n",
    "                           for var in df.columns if df[var].dtype == dtype],\n",
    "                          columns=['var', 'nunique'])\n",
    "    \n",
    "    # Sort df_cat in accending order of the number of unique value\n",
    "    df_cat = df_cat.sort_values(by='nunique', ascending=False).reset_index(drop=True)\n",
    "    \n",
    "    return df_cat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ee0Si3uAUcUM"
   },
   "source": [
    "## Data visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gb-f7eAP3OT9"
   },
   "source": [
    "### Plot the scatter plot using TSNE\n",
    "The code below shows how to plot the scatter plot using t-SNE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Ih9tkkqXonZQ"
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import os\n",
    "\n",
    "def plot_scatter_tsne(X, y, classes, labels, colors, markers, loc, dir_name, fig_name, random_seed):\n",
    "    \"\"\"\n",
    "    Plot the scatter plot using TSNE\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : the feature matrix\n",
    "    y : the target vector\n",
    "    classes : the classes in the target vector\n",
    "    labels : the labels for different classes\n",
    "    colors : the colors for different classes\n",
    "    markers : the markers for different classes\n",
    "    loc : the location of the legend\n",
    "    dir_name : the name of the directory\n",
    "    fig_name : the name of the figure\n",
    "    random_seed : the random seed\n",
    "    \"\"\"\n",
    "    \n",
    "    # Make directory\n",
    "    directory = os.path.dirname(dir_name)\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "    # Get the tsne transformed training feature matrix\n",
    "    X_embedded = TSNE(n_components=2, random_state=random_seed).fit_transform(X)\n",
    "\n",
    "    # Get the tsne dataframe\n",
    "    tsne_df = pd.DataFrame(np.column_stack((X_embedded, y)), columns=['x1', 'x2', 'y'])\n",
    "\n",
    "    # Get the data\n",
    "    data = {}\n",
    "    for class_ in classes:\n",
    "        data_x1 = [tsne_df['x1'][i] for i in range(len(tsne_df['y'])) if tsne_df['y'][i] == class_]\n",
    "        data_x2 = [tsne_df['x2'][i] for i in range(len(tsne_df['y'])) if tsne_df['y'][i] == class_]\n",
    "        data[class_] = [data_x1, data_x2]\n",
    "    \n",
    "    # The scatter plot\n",
    "    fig = plt.figure(figsize=(8, 6))\n",
    "    \n",
    "    for class_, label, color, marker in zip(classes, labels, colors, markers):\n",
    "        data_x1, data_x2 = data[class_]\n",
    "        plt.scatter(data_x1, data_x2, c=color, marker=marker, s=120, label=label)\n",
    "\n",
    "    # Set x-axis\n",
    "    plt.xlabel('x1')\n",
    "\n",
    "    # Set y-axis\n",
    "    plt.ylabel('x2')\n",
    "\n",
    "    # Set legend\n",
    "    plt.legend(loc=loc)\n",
    "\n",
    "    # Save and show the figure\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(dir_name + fig_name)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GEEsgb6qUcUO"
   },
   "source": [
    "### Separate the duplicated class from the original class\n",
    "The code below shows how to separate the duplicated class from the original class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "WnIwYWXfUcUO"
   },
   "outputs": [],
   "source": [
    "def separate_duplicate_original(X_aug_train, y_aug_train, minor_class):\n",
    "    \"\"\"\n",
    "    Separate the duplicated class from the original class\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X_aug_train : The augmented feature matrix\n",
    "    y_aug_train : The augmented target vector\n",
    "    minor_class : The minority class\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    The separated duplicated class and original class\n",
    "    \"\"\"\n",
    "\n",
    "    # Make a copy of y_aug_train\n",
    "    y_aug_dup_ori_train = np.array(y_aug_train)\n",
    "    \n",
    "    # For each sample in the augmented data\n",
    "    for i in range(X_aug_train.shape[0]):\n",
    "        # If the sample has the minor class\n",
    "        if y_aug_dup_ori_train[i] == minor_class:\n",
    "            # Flag variable, indicating whether a sample in the augmented data is the same as a sample in the original data\n",
    "            same = False\n",
    "            \n",
    "            # For each sample in the original data\n",
    "            for j in range(X_aug_train.shape[0]):\n",
    "                if j == i:\n",
    "                    continue\n",
    "\n",
    "                # If the sample has the minor class\n",
    "                if y_aug_dup_ori_train[j] == minor_class:\n",
    "                    if len(np.setdiff1d(X_aug_train[i, :], X_aug_train[j, :])) == 0:\n",
    "                        # The two samples are the same\n",
    "                        same = True\n",
    "                        break\n",
    "\n",
    "            # If the two samples are different\n",
    "            if same is False:\n",
    "                y_aug_dup_ori_train[i] = 2\n",
    "                \n",
    "    return y_aug_dup_ori_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3ixR8ZLoUcUQ"
   },
   "source": [
    "### Separate the generated class from the original class\n",
    "The code below shows how to separate the generated class from the original class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "91rbEgrYUcUQ"
   },
   "outputs": [],
   "source": [
    "def separate_generate_original(X_aug_train, y_aug_train, X_train, y_train, minor_class):\n",
    "    \"\"\"\n",
    "    Separate the generated class from the original class\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X_aug_train : The augmented feature matrix\n",
    "    y_aug_train : The augmented target vector\n",
    "    X_train : The original feature matrix\n",
    "    y_train : The original target vector\n",
    "    minor_class : The minority class\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    The separated generated class and original class\n",
    "    \"\"\"\n",
    "    \n",
    "    # Make a copy of y_aug_train\n",
    "    y_aug_gen_ori_train = np.array(y_aug_train)\n",
    "\n",
    "    # For each sample in the augmented data\n",
    "    for i in range(X_aug_train.shape[0]):\n",
    "        # If the sample has the minor class\n",
    "        if y_aug_gen_ori_train[i] == minor_class:\n",
    "            # Flag variable, indicating whether a sample in the augmented data is the same as a sample in the original data\n",
    "            same = False\n",
    "\n",
    "            # For each sample in the original data\n",
    "            for j in range(X_train.shape[0]):\n",
    "                # If the sample has the minor class\n",
    "                if y_train[j] == minor_class:\n",
    "                    if len(np.setdiff1d(X_aug_train[i, :], X_train[j, :])) == 0:\n",
    "                        # The two samples are the same\n",
    "                        same = True\n",
    "                        break\n",
    "\n",
    "            # If the two samples are different\n",
    "            if same is False:\n",
    "                y_aug_gen_ori_train[i] = 2\n",
    "                \n",
    "    return y_aug_gen_ori_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vke7rDd8UcUS"
   },
   "source": [
    "## Training, validation and test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "12eQrd4AqRK6"
   },
   "source": [
    "### Getting the predefined split cross-validator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "rYl4AGhhdT1B"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import PredefinedSplit\n",
    "\n",
    "def get_train_val_ps(X_train, y_train, X_val, y_val):\n",
    "    \"\"\"\n",
    "    Get the:\n",
    "    feature matrix and target velctor in the combined training and validation data\n",
    "    target vector in the combined training and validation data\n",
    "    PredefinedSplit\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X_train : the feature matrix in the training data\n",
    "    y_train : the target vector in the training data\n",
    "    X_val : the feature matrix in the validation data\n",
    "    y_val : the target vector in the validation data  \n",
    "\n",
    "    Return\n",
    "    ----------\n",
    "    The feature matrix in the combined training and validation data\n",
    "    The target vector in the combined training and validation data\n",
    "    PredefinedSplit\n",
    "    \"\"\"  \n",
    "\n",
    "    # Combine the feature matrix in the training and validation data\n",
    "    X_train_val = np.vstack((X_train, X_val))\n",
    "\n",
    "    # Combine the target vector in the training and validation data\n",
    "    y_train_val = np.vstack((y_train.reshape(-1, 1), y_val.reshape(-1, 1))).reshape(-1)\n",
    "\n",
    "    # Get the indices of training and validation data\n",
    "    train_val_idxs = np.append(np.full(X_train.shape[0], -1), np.full(X_val.shape[0], 0))\n",
    "\n",
    "    # The PredefinedSplit\n",
    "    ps = PredefinedSplit(train_val_idxs)\n",
    "\n",
    "    return X_train_val, y_train_val, ps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FqrtW1tFbT7Y"
   },
   "source": [
    "### Training, validation and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "JJfUeSyqbT7Y"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def training_valation_test(X_train, y_train, X_test, y_test, ps, abspath_curr, name):\n",
    "    \"\"\"\n",
    "    Training, validation and test\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X_train : the feature matrix in the training data\n",
    "    y_train : the target vector in the training data\n",
    "    X_test : the feature matrix in the test data\n",
    "    y_test : the target vector in the test data    \n",
    "    ps : the PredefinedSplit\n",
    "    abspath_curr : the absolute path of the current folder\n",
    "    name : the name of the cv_results folder\n",
    "\n",
    "    Return\n",
    "    ----------\n",
    "    The dataframe of [precision, recall, best_estimator]\n",
    "    \"\"\"    \n",
    "    \n",
    "    #************************************************************************************************\n",
    "    # Creating the directory for the cv results\n",
    "    directory = os.path.dirname(abspath_curr + name + '/')\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "        \n",
    "    #************************************************************************************************\n",
    "    # Training and validation\n",
    "\n",
    "    # The list of [best_score_, best_params_, best_estimator_] obtained by GridSearchCV\n",
    "    best_score_param_estimator_gs = []\n",
    "\n",
    "    for acronym in pipes.keys():\n",
    "        # GridSearchCV\n",
    "        gs = GridSearchCV(estimator=pipes[acronym],\n",
    "                          param_grid=param_grids[acronym],\n",
    "                          scoring='f1',\n",
    "                          n_jobs=2,\n",
    "                          cv=ps,\n",
    "                          return_train_score=True)\n",
    "\n",
    "        # Fit the pipeline\n",
    "        gs = gs.fit(X_train, y_train)\n",
    "\n",
    "        # Update best_score_param_estimator_gs\n",
    "        best_score_param_estimator_gs.append([gs.best_score_, gs.best_params_, gs.best_estimator_])\n",
    "\n",
    "        # Sort cv_results in ascending order of 'rank_test_score' and 'std_test_score'\n",
    "        cv_results = pd.DataFrame.from_dict(gs.cv_results_).sort_values(by=['rank_test_score', 'std_test_score'])\n",
    "\n",
    "        # Get the important columns in cv_results\n",
    "        important_columns = ['rank_test_score',\n",
    "                             'mean_test_score', \n",
    "                             'std_test_score', \n",
    "                             'mean_train_score', \n",
    "                             'std_train_score',\n",
    "                             'mean_fit_time', \n",
    "                             'std_fit_time',                        \n",
    "                             'mean_score_time', \n",
    "                             'std_score_time']\n",
    "\n",
    "        # Move the important columns ahead\n",
    "        cv_results = cv_results[important_columns + sorted(list(set(cv_results.columns) - set(important_columns)))]\n",
    "\n",
    "        # Write cv_results file\n",
    "        cv_results.to_csv(path_or_buf=abspath_curr + name + '/' + acronym + '.csv', index=False)\n",
    "\n",
    "    #************************************************************************************************\n",
    "    # Test\n",
    "\n",
    "    # The list of [precision, recall, fscore, auc, best_estimator]\n",
    "    precision_recall_fscore_auc_best_estimator = []\n",
    "\n",
    "    for best_score, best_param, best_estimator in best_score_param_estimator_gs:\n",
    "        # Get the prediction\n",
    "        y_pred = best_estimator.predict(X_test)\n",
    "\n",
    "        # Get the precision, recall, fscore, support\n",
    "        precision, recall, fscore, support = precision_recall_fscore_support(y_test, y_pred)\n",
    "\n",
    "        # Get the auc\n",
    "        auc = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "        # Update precision_recall_fscore_auc_best_estimator\n",
    "        precision_recall_fscore_auc_best_estimator.append([precision, recall, fscore, auc, best_estimator])\n",
    "\n",
    "    # Return precision_recall_fscore_best_estimator\n",
    "    return pd.DataFrame(precision_recall_fscore_auc_best_estimator,\n",
    "                        columns=['Precision', 'Recall', 'F1-score', 'AUC', 'Model'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KeR3L0fsmq1G"
   },
   "source": [
    "# Reference\n",
    "- The code in this notebook is from the code examples developed in the following course of [Yuxiao Huang](https://sites.google.com/view/yuxiaohuang/home?authuser=0) at George Washington University:\n",
    "    - [Popular Machine Learning Methods: Idea, Math and Practice](https://github.com/yuxiaohuang/teaching/tree/master/gwu/machine_learning_I)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "pmlm_utilities_shallow.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
